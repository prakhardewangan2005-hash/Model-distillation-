project:
  name: "MODEL DISTILLATION LAB"
  outputs_dir: "outputs"
  cache_dir: "outputs/cache"
  run_name: null            # if null, autogenerated
  deterministic: true
  seed: 42

data:
  dataset_name: "glue"
  dataset_config: "sst2"
  text_field: "sentence"
  label_field: "label"
  num_labels: 2
  # quick demo uses a small subset to keep CPU time low
  quick_train_samples: 800
  quick_eval_samples: 400
  full_train_samples: 5000
  full_eval_samples: 2000
  max_length: 128
  streaming: true           # still materializes subset; good for large datasets

models:
  teacher_name: "distilbert-base-uncased-finetuned-sst-2-english"
  student_name: "prajjwal1/bert-tiny"
  # if you want a larger student:
  # student_name: "distilbert-base-uncased"
  teacher_revision: null
  student_revision: null

train:
  mode: "quick"             # quick | full
  batch_size: 16
  eval_batch_size: 32
  lr: 5.0e-5
  weight_decay: 0.01
  num_epochs: 1
  max_steps_quick: 80
  max_steps_full: 600
  grad_accum_steps: 1
  warmup_ratio: 0.06
  log_every: 10
  eval_every: 40
  save_every: 80

distill:
  alpha: 0.6
  temperature: 2.0

runtime:
  device: "cpu"
  num_workers: 0
  timeout_seconds: 30
  retries: 3
  backoff_base_seconds: 1.0

bench:
  samples: 200
  batch_size: 16
  warmup_batches: 5
